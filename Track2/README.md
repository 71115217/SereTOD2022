# SereTOD Track2: Task-Oriented Dialog Systems
This repository contains the evaluation and baseline codes for SereTOD Track2.

Most existing TOD systems require not only large amounts of annotations of dialog states and dialog acts (if used), but also a global knowledge base (KB) covering all public knowledge and all personal information in the domain, which are both difficult to obtain at the research stage. Compared with previous work, the task in Track2 has two main characteristics:
* There's no global KB but only a local KB for each dialog, representing the unique information for each user, e.g., the user's package plan and remaining phone charges.
*  Only a proportion of the dialogs is annotated with intents and local KBs. The teams are encouraged to utilize a mix of labeled and unlabeled dialogs to build a TOD system.

# Task Definition
The basic task for the TOD system is, for each dialog turn, given the dialog history, the user utterance and the local KB, to predict the user intent, query the local KB and generate appropriate system intent and response according to the queried information. 
For every labeled dialog, the annotations consist of the user intents, system intents and a local KB obtained by integrating the annotations of entities and triples in Track1. 
For every unlabeled dialog, those annotations are missing. However, the local KBs of unlabeled dialogs can be constructed by applying the information extraction model in Track1 to extract entities and triples. And the missing intent annotations can also be supplemented by semi-supervised methods such as pseudo labeling.
# Evaluation
In order to measure the performance of TOD systems, both automatic evaluation and human evaluation will be conducted. 
For automatic evaluation, metrics include Precision/Recall/F1 score, Success rate and BLEU score.  P/R/F1 are calculated for both predicted user intents and system intents.
Success rate is the percentage of generated dialogs that achieve user goals. BLEU score evaluates the fluency of generated responses.

We will also perform human evaluation for different TOD systems, where the real users interact with those systems according to randomly given goals. For each dialog, the user will score the system in the following 4 aspects:
* Success. This metric measures if the system has successfully fulfilled the user's goal
* Understanding. This metric measures whether the system’s response shows that the system is able to understand the goal and intent of the user
* Coherency. This metric measures whether the system’s response is logically coherent with the dialogue context
* The metric measures the fluency of the system’s response

There are only 3 different scores for each aspect, 0, 1 and 2, respectively indicating three degrees: not at all, partially and completely.

The average scores from automatic evaluation and human evaluation will be the main ranking basis on leaderboard.
We will provide the following scripts and tools for the participants: 1) A baseline system; 2) Evaluation scripts to calculate the corpus-based metrics.
# Data
In this challenge task, participants will use [SereTOD dataset](../data/) to build a TOD system. Note that there are only annotations of intents, entities and triples in the dataset, so it's neccessary to **extract the local KB and user goal** for each dialogue. The extraction script can be seen in [baseline](./baseline/).

In the test phase, participants will be evaluated on the results generated by their models for the hidden test set.
The test set will be on the same domains, entities and locales as the training set.

